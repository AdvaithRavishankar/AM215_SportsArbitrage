# Sports Betting Arbitrage Analysis

![CI](https://github.com/AdvaithRavishankar/AM215_SportsArbitrage/actions/workflows/python-app.yml/badge.svg)
![Docs](https://github.com/AdvaithRavishankar/AM215_SportsArbitrage/actions/workflows/docs.yml/badge.svg)

A comprehensive sports betting analysis and arbitrage system for NFL games using multiple prediction models and arbitrage detection.

## Project Overview

This project extends our work from Project 1 on sports odds betting. We use multiple models to predict NFL game outcomes and identify arbitrage opportunities across different sportsbooks to maximize ROI while hedging risk.

## Reproducible Setup

- **Python package deps:** managed in `pyproject.toml`.
- **Pinned analysis env:** `environment.yml` (conda/mamba) for exact versions; alternatively `pip install -r requirements.txt`.
- **Dev/test extras:** `pip install -e .[dev,test]` for linting, type checking, and tests.
- **Determinism:** RNG is routed through `numpy.random.default_rng`; the active seed is logged at run start.

## Running the Analysis

From the repo root (paths are now robust to working directory):

```bash
python src/run.py
```

Artifacts land in `results/` (figures, CSV metrics).

## Docker

Reproducible container build:
```bash
docker build -t sports-arbitrage .
docker run --rm -v "$(pwd)/results:/app/results" sports-arbitrage
```

## Testing, Linting, Type Checking

```bash
pytest               # unit + integration tests
ruff check src tests # lint
black --check src tests
mypy src/sports_arbitrage
```
CI runs these on `push`/`pull_request` to `main`/`develop`.

## Documentation

- Source: `docs/` (Sphinx).
- Build locally: `sphinx-build -b html docs docs/_build/html`
- CI workflow `docs.yml` builds docs and uploads an artifact; publish to GitHub Pages by enabling Pages to point at that artifact.

## Performance & Profiling

- Profiling artifact: `profiling/profile_summary.txt` (generated via `python scripts/profile_analysis.py --output profiling/profile_stats.prof --limit 30 --sort cumulative`).
- Bottleneck: CPU-bound in `calculate_markowitz_roi` (SciPy SLSQP) and PNG writing. Concurrency not added because optimization is already CPU-bound per model and dataset is modest; parallelizing would add overhead for small fold counts.
- For further gains, consider reducing Markowitz iterations or caching per-day optimization.

## LLM Usage

This repository was updated with assistance from an LLM (OpenAI ChatGPT/Codex) for refactoring, documentation, and tooling. All changes were reviewed and integrated manually.

### Key Features

- **Multiple Prediction Models**: ELO, Rank Centrality, XGBoost, Random Forest
- **3-Fold Rolling Window Cross-Validation**: Time-series aware validation
- **Arbitrage Detection**: Find risk-free betting opportunities across sportsbooks
- **ROI Calculation**: Measure profitability of betting strategies
- **Comprehensive Visualizations**: Model comparisons, calibration curves, feature importance
- **Modular Design**: Clean separation of concerns, ready for PyPI

## Project Structure

```
AM215_SportsArbitrage/
├── src/
│   ├── sports_arbitrage/
│   │   ├── __init__.py
│   │   ├── utils.py              # Utility functions
│   │   ├── plotting.py           # Visualization functions
│   │   └── models/
│   │       ├── __init__.py
│   │       ├── elo.py            # ELO rating system
│   │       ├── rank_centrality.py    # Graph-based ranking
│   │       ├── xgboost_model.py      # XGBoost classifier
│   │       └── random_forest.py      # Random Forest classifier
│   ├── run.py                    # Main execution script
│   └── ODDSAPI_historical_extraction.ipynb  # Data extraction notebook
├── data/
│   └── odds_2020_2024_combined.csv  # Odds data
├── results/                      # Generated by run.py
│   ├── figures/                  # Generated plots
│   ├── cv_results.csv            # Cross-validation results
│   ├── test_results.csv          # Test set results
│   ├── roi_results.csv           # ROI calculations
│   └── arbitrage_opportunities.csv
├── setup.py                      # Package setup for PyPI
├── requirements.txt              # Dependencies
└── README.md                     # This file
```

## Branching & Workflow

- Use feature branches (`feat/*`, `fix/*`) and PRs into `main` with at least one reviewer.
- Protect `main` in GitHub settings to require CI passes before merging.


## Models

### 1. ELO Rating System

Traditional ELO rating system adapted for NFL games. Updates team ratings based on game outcomes with home advantage consideration.

**Key Parameters**:
- `k_factor=20`: Maximum rating change per game
- `initial_rating=1500`: Starting rating for new teams
- `home_advantage=50`: Points added to home team

### 2. Rank Centrality

Graph-based ranking using PageRank algorithm. Constructs a network where teams are nodes and game results create weighted directed edges.

**Key Parameters**:
- `method='pagerank'`: Centrality algorithm
- `damping_factor=0.85`: PageRank damping factor
- `home_advantage=0.05`: Home team boost

### 3. XGBoost

Gradient boosting with engineered features including win rates, recent form, and head-to-head history.

**Key Parameters**:
- `n_estimators=100`: Number of boosting rounds
- `max_depth=6`: Maximum tree depth
- `learning_rate=0.1`: Boosting learning rate

### 4. Random Forest

Ensemble of decision trees with additional feature engineering including recent form.

**Key Parameters**:
- `n_estimators=100`: Number of trees
- `max_depth=10`: Maximum tree depth
- `min_samples_split=5`: Minimum samples to split

## Arbitrage Strategy

The system detects arbitrage opportunities by:

1. Finding the best odds for each outcome across all sportsbooks
2. Calculating implied probabilities
3. Identifying cases where total probability < 1 (arbitrage exists)
4. Computing optimal stake percentages for guaranteed profit

Example arbitrage output:
```
Game: Patriots @ Bills
  Home: DraftKings (-110) - Stake: 52.4%
  Away: FanDuel (+130) - Stake: 47.6%
  ROI: 2.3%
```

## Evaluation Metrics

- **Accuracy**: Classification accuracy
- **Log Loss**: Logarithmic loss (lower is better)
- **Brier Score**: Calibration metric (lower is better)
- **ROC AUC**: Area under ROC curve
- **ROI**: Return on investment percentage
- **Profit**: Total profit/loss in dollars

## Rolling Window Cross-Validation

The project uses time-series aware validation:

1. Data is sorted chronologically
2. 3 folds are created with expanding training windows
3. Each fold trains on historical data and tests on future games
4. Final evaluation uses last 20% of data as holdout test set

This approach respects temporal ordering and prevents data leakage.

## Contributing

This project follows standard software engineering practices:

- **Modular Design**: Each model in separate file
- **Type Hints**: For better code clarity
- **Documentation**: Comprehensive docstrings
- **Clean Separation**: Utils, plotting, and models separated
- **PyPI Ready**: Includes `__init__.py` files and proper structure

## Requirements

See `requirements.txt` for full list. Main dependencies:

- numpy
- pandas
- scikit-learn
- xgboost
- networkx
- matplotlib
- seaborn

## License

MIT License - See LICENSE file for details

## Authors

AM215 Sports Arbitrage Team

## Acknowledgments

- Project 1: Sports Odds Betting (foundation)
- The Odds API for historical odds data
- NFL Python API for game data
